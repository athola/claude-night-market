"""Tests for review-analyst agent functionality.

This module tests the autonomous review agent capabilities and workflow integration,
following TDD/BDD principles and testing all agent scenarios.
"""

import time
from datetime import UTC, datetime
from unittest.mock import Mock

import pytest

# Constants for PLR2004 magic values
ZERO_POINT_FIVE = 0.5
TWO = 2
TWO_POINT_ZERO = 2.0
THREE = 3
FOUR = 4
TWENTY = 20


class TestReviewAnalystAgent:
    """Feature: Review analyst agent conducts autonomous reviews.

    As a user delegating review work
    I want the agent to follow imbue methodology
    So that reviews are consistent and thorough
    """

    @pytest.fixture
    def mock_review_analyst_agent_content(self):
        """Mock review-analyst agent content from agents/review-analyst.md."""
        return {
            "name": "review-analyst",
            "description": (
                "Autonomous agent for conducting structured reviews "
                "with evidence gathering"
            ),
            "tools": ["Read", "Glob", "Grep", "Bash"],
            "capabilities": [
                "context_establishment",
                "scope_discovery",
                "evidence_gathering",
                "finding_categorization",
                "deliverable_generation",
            ],
            "integrates_with": [
                "review-core",
                "evidence-logging",
                "structured-output",
                "diff-analysis",
            ],
            "workflow_steps": [
                "Initialize",
                "Discover",
                "Analyze",
                "Categorize",
                "Format",
                "Report",
            ],
        }

    @pytest.fixture
    def sample_agent_session(self):
        """Sample agent session context."""
        return {
            "session_id": "agent-session-123",
            "initiated_at": datetime.now(UTC).isoformat(),
            "focus": "security_review",
            "target": "src/auth/",
            "tools_available": ["Read", "Glob", "Grep", "Bash"],
            "completed_steps": [],
        }

    @pytest.fixture
    def sample_agent_findings(self):
        """Sample findings generated by review-analyst agent."""
        return [
            {
                "id": "F1",
                "title": "SQL injection vulnerability in login function",
                "description": (
                    "User input directly concatenated into SQL query "
                    "without sanitization"
                ),
                "severity": "Critical",
                "category": "Security",
                "file": "src/auth/login.py",
                "line": 45,
                "evidence_refs": ["E1"],  # Each finding gets one evidence ref
                "recommendation": (
                    "Use parameterized queries or ORM to prevent SQL injection"
                ),
                "cvss_score": 9.8,
                "impact": "Complete database compromise possible",
            },
            {
                "id": "F2",
                "title": "Password stored in plaintext",
                "description": "User passwords stored without hashing or encryption",
                "severity": "Critical",
                "category": "Security",
                "file": "src/auth/models.py",
                "line": 23,
                "evidence_refs": ["E2"],  # Each finding gets one evidence ref
                "recommendation": (
                    "Implement proper password hashing using bcrypt or Argon2"
                ),
                "cvss_score": 9.0,
                "impact": "User credentials exposed in data breach",
            },
            {
                "id": "F3",
                "title": "Missing rate limiting on authentication endpoint",
                "description": (
                    "Authentication API lacks rate limiting, "
                    "vulnerable to brute force attacks"
                ),
                "severity": "High",
                "category": "Security",
                "file": "src/auth/api.py",
                "line": 15,
                "evidence_refs": ["E3"],  # Each finding gets one evidence ref
                "recommendation": "Implement rate limiting with exponential backoff",
                "cvss_score": 7.5,
                "impact": "Account takeover through credential stuffing",
            },
        ]

    @pytest.mark.unit
    def test_agent_follows_imbue_workflow(
        self,
        mock_claude_tools,
        sample_agent_session,
    ) -> None:
        """Scenario: Agent uses all imbue skills correctly.

        Given a review-analyst dispatch
        When conducting review
        Then it should use review-core for scaffolding
        And evidence-logging for citations
        And structured-output for reporting.
        """
        # Arrange - track skill usage
        used_skills = []
        skill_contexts = {}

        def mock_skill_usage(skill_name, context) -> str:
            used_skills.append(skill_name)
            skill_contexts[skill_name] = context.copy()
            return f"{skill_name} executed successfully"

        mock_claude_tools["Skill"] = Mock(side_effect=mock_skill_usage)

        # Act - simulate agent workflow execution
        agent_context = sample_agent_session.copy()

        # Step 1: Initialize with review-core
        mock_claude_tools["Skill"](
            "review-core",
            {
                "agent": "review-analyst",
                "focus": agent_context["focus"],
                "target": agent_context["target"],
            },
        )
        agent_context["completed_steps"].append("workflow_scaffolded")

        # Step 2: Set up evidence logging
        mock_claude_tools["Skill"](
            "evidence-logging",
            {"session_id": agent_context["session_id"], "agent": "review-analyst"},
        )
        agent_context["completed_steps"].append("evidence_logging_ready")

        # Step 3: Analyze changes with diff-analysis
        mock_claude_tools["Skill"](
            "diff-analysis",
            {
                "baseline": "main",
                "target": agent_context["target"],
                "focus": agent_context["focus"],
            },
        )
        agent_context["completed_steps"].append("changes_analyzed")

        # Step 4: Format output with structured-output
        mock_claude_tools["Skill"](
            "structured-output",
            {
                "template_type": "security_review",
                "findings_count": 3,  # Would be populated from actual analysis
                "evidence_count": 6,
            },
        )
        agent_context["completed_steps"].append("report_formatted")

        # Assert
        expected_skills = [
            "review-core",
            "evidence-logging",
            "diff-analysis",
            "structured-output",
        ]
        assert len(used_skills) == FOUR
        assert all(skill in used_skills for skill in expected_skills)

        # Verify skill integration
        assert skill_contexts["evidence-logging"]["session_id"] == "agent-session-123"
        assert skill_contexts["review-core"]["focus"] == "security_review"
        assert skill_contexts["diff-analysis"]["target"] == "src/auth/"
        assert skill_contexts["structured-output"]["template_type"] == "security_review"

    @pytest.mark.unit
    def test_agent_gathers_reproducible_evidence(
        self,
        mock_claude_tools,
        sample_agent_findings,
    ) -> None:
        """Scenario: Agent captures reproducible evidence.

        Given an agent conducting review
        When analyzing artifacts
        Then every finding should have evidence reference
        And commands should be logged.
        """
        # Arrange - simulate evidence gathering
        evidence_log = {
            "session_id": "agent-session-123",
            "evidence": [],
            "citations": [],
        }

        # Mock tool usage for evidence gathering
        mock_claude_tools["Bash"].side_effect = [
            # Grep for SQL patterns
            # nosec: S608 - Mock test data demonstrating SQL injection pattern
            "src/auth/login.py:45: query = \"SELECT * FROM users WHERE email = '"
            + "test@example.com"
            + "'",
            # Grep for password storage
            "src/auth/models.py:23: password = user_data['password']",
            # Grep for endpoints
            "src/auth/api.py:15: @app.route('/login', methods=['POST'])",
        ]

        mock_claude_tools["Read"].side_effect = [
            # SQL injection vulnerable code
            "def authenticate_user(email, password):\n    "
            "# SQL injection vulnerable code\n    "
            'query = "SELECT * FROM users WHERE email = \'" + email + "\'"\n    '
            "cursor.execute(query)",
            # Password stored in plaintext
            "class User:\n    def __init__(self, data):\n        "
            "self.email = data['email']\n        "
            "self.password = data['password']  # Stored in plaintext",
            # No rate limiting
            "@app.route('/login', methods=['POST'])\ndef login():\n    "
            "# No rate limiting implemented\n    "
            "return authenticate()",
        ]

        # Act - gather evidence for findings
        for i, finding in enumerate(sample_agent_findings):
            # Simulate evidence collection for each finding
            search_pattern = self._get_search_pattern_for_finding(finding)
            search_results = mock_claude_tools["Grep"](search_pattern)

            # Read the relevant file
            mock_claude_tools["Read"](finding["file"])

            # Log evidence
            evidence_item = {
                "id": f"E{i + 1}",
                "command": f"grep -r '{search_pattern}' src/",
                "output": search_results,
                "timestamp": datetime.now(UTC).isoformat(),
                "file": finding["file"],
                "line": finding["line"],
            }
            evidence_log["evidence"].append(evidence_item)

        # Assert
        assert len(evidence_log["evidence"]) == len(sample_agent_findings)

        # Verify each finding has evidence references
        for finding in sample_agent_findings:
            assert len(finding["evidence_refs"]) > 0
            # Verify evidence references correspond to logged evidence
            for ref in finding["evidence_refs"]:
                evidence_exists = any(e["id"] == ref for e in evidence_log["evidence"])
                assert (
                    evidence_exists
                ), f"Evidence reference {ref} not found in evidence log"

        # Verify evidence completeness
        for evidence in evidence_log["evidence"]:
            assert "command" in evidence
            assert "output" in evidence
            assert "timestamp" in evidence
            assert "file" in evidence
            assert "line" in evidence

    def _get_search_pattern_for_finding(self, finding):
        """Get appropriate search pattern for finding type."""
        if "SQL injection" in finding["title"]:
            return "SELECT.*email"
        if "password" in finding["title"].lower():
            return "password.*=.*\\["
        if "rate limiting" in finding["title"].lower():
            return "@app.route.*login"
        return finding["title"].lower().split()[0]

    @pytest.mark.unit
    def test_agent_categorizes_findings_by_severity(
        self, sample_agent_findings
    ) -> None:
        """Scenario: Agent categorizes findings by severity with justification.

        Given multiple findings identified
        When categorizing findings
        Then severity levels should be justified
        And follow consistent criteria.
        """
        # Arrange & Act - analyze and categorize findings
        categorized_findings = {"Critical": [], "High": [], "Medium": [], "Low": []}

        severity_criteria = {
            "Critical": {
                "description": (
                    "Vulnerabilities that can be exploited without user interaction "
                    "and result in complete system compromise"
                ),
                "examples": [
                    "Remote code execution",
                    "SQL injection",
                    "Authentication bypass",
                ],
                "cvss_range": (9.0, 10.0),
            },
            "High": {
                "description": (
                    "Vulnerabilities that require some user interaction or "
                    "privileges and result in significant impact"
                ),
                "examples": [
                    "Privilege escalation",
                    "Data exposure",
                    "Denial of service",
                ],
                "cvss_range": (7.0, 8.9),
            },
            "Medium": {
                "description": (
                    "Vulnerabilities with limited impact or requiring "
                    "complex exploitation"
                ),
                "examples": ["Information disclosure", "Weak cryptography"],
                "cvss_range": (4.0, 6.9),
            },
            "Low": {
                "description": (
                    "Vulnerabilities with minimal impact or requiring local access"
                ),
                "examples": ["Informational findings", "Best practice violations"],
                "cvss_range": (0.1, 3.9),
            },
        }

        # Categorize findings and add justifications
        for finding in sample_agent_findings:
            severity = finding["severity"]

            # Add severity justification
            cvss_score = finding.get("cvss_score", 0)
            criteria = severity_criteria[severity]

            categorized_finding = finding.copy()
            categorized_finding["severity_justification"] = {
                "rationale": criteria["description"],
                "cvss_score": cvss_score,
                "matches_criteria": cvss_score >= criteria["cvss_range"][0]
                and cvss_score <= criteria["cvss_range"][1],
            }

            categorized_findings[severity].append(categorized_finding)

        # Assert
        assert len(categorized_findings["Critical"]) == TWO
        assert len(categorized_findings["High"]) == 1

        # Verify severity justifications
        for severity, findings in categorized_findings.items():
            for finding in findings:
                assert "severity_justification" in finding
                assert finding["severity_justification"]["rationale"] is not None
                assert "cvss_score" in finding["severity_justification"]
                assert finding["severity_justification"]["matches_criteria"] is True

        # Check specific justifications
        critical_finding = categorized_findings["Critical"][0]  # SQL injection
        assert critical_finding["severity_justification"]["cvss_score"] == 9.8
        assert critical_finding["severity_justification"]["matches_criteria"] is True

    @pytest.mark.unit
    def test_agent_generates_actionable_recommendations(
        self, sample_agent_findings
    ) -> None:
        """Scenario: Agent generates actionable and specific recommendations.

        Given security findings identified
        When creating recommendations
        Then they should be specific and actionable
        With implementation guidance.
        """
        # Arrange & Act - enhance findings with actionable recommendations
        enhanced_findings = []

        for finding in sample_agent_findings:
            enhanced_finding = finding.copy()

            # Add implementation guidance
            enhanced_finding["implementation_guidance"] = (
                self._generate_implementation_guidance(finding)
            )

            # Add verification steps
            enhanced_finding["verification_steps"] = self._generate_verification_steps(
                finding,
            )

            # Add estimated effort
            enhanced_finding["estimated_effort"] = self._estimate_effort(finding)

            # Add dependencies
            enhanced_finding["dependencies"] = self._identify_dependencies(finding)

            enhanced_findings.append(enhanced_finding)

        # Assert
        for finding in enhanced_findings:
            assert "implementation_guidance" in finding
            assert "verification_steps" in finding
            assert "estimated_effort" in finding
            assert "dependencies" in finding

            # Verify recommendation specificity
            guidance = finding["implementation_guidance"]
            assert len(guidance) > TWENTY  # Detailed guidance
            assert any(
                keyword in guidance.lower()
                for keyword in ["use", "implement", "add", "configure"]
            )

            # Verify verification steps
            verification = finding["verification_steps"]
            assert len(verification) >= 1  # At least one verification step
            assert any(
                keyword in " ".join(verification).lower()
                for keyword in ["test", "verify", "check", "confirm"]
            )

        # Check specific findings
        sql_injection_finding = next(
            f for f in enhanced_findings if "SQL injection" in f["title"]
        )
        assert (
            "parameterized" in sql_injection_finding["implementation_guidance"].lower()
        )
        assert len(sql_injection_finding["verification_steps"]) >= TWO

    def _generate_implementation_guidance(self, finding):
        """Generate specific implementation guidance for finding."""
        guidance_map = {
            "SQL injection": (
                "Replace string concatenation with parameterized queries using "
                "prepared statements or ORM. Example: cursor.execute('SELECT * FROM "
                "users WHERE email = %s', (email,))"
            ),
            "password": (
                "Implement strong password hashing using bcrypt or Argon2. Store only "
                "salted hashes, never plaintext passwords. Example: "
                "bcrypt.hashpw(password.encode(), bcrypt.gensalt())"
            ),
            "rate limiting": (
                "Implement rate limiting using exponential backoff. Track attempts per "
                "IP/user and delay responses after threshold exceeded. Use libraries "
                "like flask-limiter or redis for distributed limiting"
            ),
        }
        # Match by finding keywords in the title
        title = finding["title"].lower()
        for key, guidance in guidance_map.items():
            if key.lower() in title:
                return guidance
        return "Use best practices to address the identified issue"

    def _generate_verification_steps(self, finding):
        """Generate verification steps for finding."""
        return [
            "Run unit tests with malicious input to verify fix",
            "Perform manual penetration testing of the endpoint",
            "Review code changes for proper error handling",
        ]

    def _estimate_effort(self, finding):
        """Estimate implementation effort in person-days."""
        effort_map = {"Critical": 2, "High": 1, "Medium": 0.5, "Low": 0.25}
        return effort_map.get(finding["severity"], 1)

    def _identify_dependencies(self, finding):
        """Identify dependencies for implementing the fix."""
        if "SQL injection" in finding["title"]:
            return ["Database schema review", "Application deployment coordination"]
        if "password" in finding["title"]:
            return ["User migration plan", "Password reset functionality"]
        if "rate limiting" in finding["title"]:
            return ["Rate limiting service setup", "Monitoring configuration"]
        return []

    @pytest.mark.unit
    def test_agent_produces_consistent_report_structure(
        self, sample_agent_findings
    ) -> None:
        """Scenario: Agent produces reports with consistent template structure.

        Given completed analysis with findings
        When generating final report
        Then it should follow imbue structured-output standards
        With all required sections.
        """
        # Arrange & Act - generate structured report
        report = {
            "metadata": {
                "agent": "review-analyst",
                "session_id": "agent-session-123",
                "timestamp": datetime.now(UTC).isoformat(),
                "focus": "security_review",
                "target": "src/auth/",
                "total_findings": len(sample_agent_findings),
            },
            "executive_summary": self._generate_executive_summary(
                sample_agent_findings,
            ),
            "findings_by_severity": self._organize_findings_by_severity(
                sample_agent_findings,
            ),
            "detailed_findings": sample_agent_findings,
            "action_items": self._generate_action_items(sample_agent_findings),
            "evidence_appendix": self._create_evidence_appendix(),
            "methodology": "imbue review methodology with evidence logging and structured output",
        }

        # Assert required sections exist
        required_sections = [
            "metadata",
            "executive_summary",
            "findings_by_severity",
            "detailed_findings",
            "action_items",
            "evidence_appendix",
        ]

        for section in required_sections:
            assert section in report

        # Verify metadata completeness
        metadata = report["metadata"]
        assert metadata["agent"] == "review-analyst"
        assert metadata["session_id"] == "agent-session-123"
        assert metadata["total_findings"] == THREE
        assert "timestamp" in metadata

        # Verify findings organization
        assert len(report["findings_by_severity"]["Critical"]) == TWO
        assert len(report["findings_by_severity"]["High"]) == 1

        # Verify action items
        action_items = report["action_items"]
        assert len(action_items) >= THREE  # At least one per finding
        assert all("priority" in item for item in action_items)
        assert all("description" in item for item in action_items)

        # Verify evidence appendix
        evidence_appendix = report["evidence_appendix"]
        assert "total_evidence_items" in evidence_appendix
        assert "evidence_references" in evidence_appendix

    def _generate_executive_summary(self, findings) -> str:
        """Generate executive summary for findings."""
        critical_count = len([f for f in findings if f["severity"] == "Critical"])
        high_count = len([f for f in findings if f["severity"] == "High"])

        return f"""Security review identified {len(findings)} findings requiring immediate attention.
{critical_count} critical vulnerabilities pose severe security risks including potential database compromise.
{high_count} high-severity issues could enable account takeover attacks.
Immediate remediation of critical findings is essential before production deployment."""

    def _organize_findings_by_severity(self, findings):
        """Organize findings by severity level."""
        organized = {"Critical": [], "High": [], "Medium": [], "Low": []}
        for finding in findings:
            if finding["severity"] in organized:
                organized[finding["severity"]].append(
                    {
                        "id": finding["id"],
                        "title": finding["title"],
                        "file": finding["file"],
                        "impact": finding.get("impact", "Security impact"),
                    },
                )
        return organized

    def _generate_action_items(self, findings):
        """Generate prioritized action items from findings."""
        action_items = []
        for finding in findings:
            action_items.append(
                {
                    "id": f"A{finding['id'][1:]}",  # Convert F1 to A1
                    "description": finding["recommendation"],
                    "priority": finding["severity"],
                    "related_finding": finding["id"],
                    "estimated_effort": self._estimate_effort(finding),
                },
            )
        return action_items

    def _create_evidence_appendix(self):
        """Create evidence appendix structure."""
        return {
            "total_evidence_items": 6,
            "evidence_references": ["E1", "E2", "E3", "E4", "E5", "E6"],
            "evidence_log_available": True,
            "reproducible_commands": True,
        }

    @pytest.mark.unit
    def test_agent_handles_tool_failures_gracefully(self, mock_claude_tools) -> None:
        """Scenario: Agent handles tool failures gracefully.

        Given tool execution failures during review
        When conducting analysis
        Then it should handle errors and continue analysis
        And report limitations in findings.
        """
        # Arrange - simulate tool failures
        mock_claude_tools["Read"].side_effect = [
            "File content",  # First read succeeds
            PermissionError("Permission denied"),  # Second read fails
            "File content",  # Third read succeeds
        ]

        mock_claude_tools["Grep"].side_effect = [
            "Search results",  # First search succeeds
            "Timeout error",  # Second search fails
        ]

        # Act - simulate agent handling tool failures
        findings = []
        errors_encountered = []

        # Simulate analyzing multiple files with potential failures
        test_files = ["file1.py", "file2.py", "file3.py"]

        for i, file_path in enumerate(test_files):
            try:
                # Try to read file
                mock_claude_tools["Read"](file_path)

                # Try to search for patterns
                mock_claude_tools["Grep"]("pattern", file_path)

                # Create finding if successful
                findings.append(
                    {"id": f"F{i + 1}", "file": file_path, "analysis_successful": True},
                )

            except Exception as e:
                errors_encountered.append(
                    {"file": file_path, "error": str(e), "impact": "limited_analysis"},
                )

        # Generate report with limitations noted
        analysis_report = {
            "findings": findings,
            "errors_encountered": errors_encountered,
            "analysis_limitations": [
                "Some files could not be analyzed due to permission errors",
                "Pattern matching was incomplete for certain files",
            ],
            "confidence_level": "medium" if errors_encountered else "high",
        }

        # Assert
        assert len(findings) >= 1  # At least some files analyzed successfully
        assert len(errors_encountered) >= 1  # At least one error encountered
        assert "analysis_limitations" in analysis_report
        assert analysis_report["confidence_level"] == "medium"
        assert all("error" in error for error in errors_encountered)

    @pytest.mark.performance
    def test_agent_performance_with_large_codebases(self) -> None:
        """Scenario: Agent performs efficiently with large codebases.

        Given many files to analyze
        When conducting review
        Then it should complete analysis in reasonable time
        With focused analysis on high-risk areas.
        """
        # Arrange - simulate large codebase
        large_codebase = {
            "total_files": 500,
            "high_risk_files": [f"src/auth/file_{i}.py" for i in range(20)],
            "medium_risk_files": [f"src/api/file_{i}.py" for i in range(50)],
            "low_risk_files": [f"src/utils/file_{i}.py" for i in range(100)],
        }

        # Act - measure analysis performance
        start_time = time.time()

        # Simulate agent's focused analysis strategy
        analysis_strategy = {
            "focus_areas": ["authentication", "data_handling", "api_endpoints"],
            "file_prioritization": [],
            "analysis_results": {},
        }

        # Prioritize high-risk files for detailed analysis
        analysis_strategy["file_prioritization"] = (
            large_codebase["high_risk_files"]
            + large_codebase["medium_risk_files"][:20]  # Sample of medium risk
            + large_codebase["low_risk_files"][:10]  # Sample of low risk
        )

        # Simulate analysis with token conservation
        for file_path in analysis_strategy["file_prioritization"]:
            # Simulate quick risk assessment
            if "auth" in file_path:
                risk_level = "high"
                analysis_depth = "deep"
            elif "api" in file_path:
                risk_level = "medium"
                analysis_depth = "moderate"
            else:
                risk_level = "low"
                analysis_depth = "light"

            analysis_strategy["analysis_results"][file_path] = {
                "risk_level": risk_level,
                "analysis_depth": analysis_depth,
                "findings": 1
                if risk_level == "high"
                else 0.3
                if risk_level == "medium"
                else 0.1,
            }

        end_time = time.time()

        # Generate summary
        total_findings = sum(
            result["findings"]
            for result in analysis_strategy["analysis_results"].values()
        )
        performance_summary = {
            "processing_time": end_time - start_time,
            "files_analyzed": len(analysis_strategy["file_prioritization"]),
            "total_files_in_repo": large_codebase["total_files"],
            "analysis_coverage": len(analysis_strategy["file_prioritization"])
            / large_codebase["total_files"],
            "estimated_findings": int(total_findings),
            "efficiency_achieved": True,
        }

        # Assert
        assert (
            performance_summary["processing_time"] < TWO_POINT_ZERO
        )  # Should complete in under 2 seconds
        assert (
            performance_summary["files_analyzed"] < large_codebase["total_files"]
        )  # Focused analysis
        assert (
            performance_summary["analysis_coverage"] < ZERO_POINT_FIVE
        )  # Less than 50% coverage (focused approach)
        assert performance_summary["efficiency_achieved"] is True
