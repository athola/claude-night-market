# Pressure Testing for Skills

## Overview

Pressure tests validate that skills work under adversarial conditions - scenarios designed to tempt agents into violating skill principles. A skill that only works when conditions are easy is not robust.

**Core principle:** Skills must resist rationalization under pressure. If an agent can talk itself out of following the skill when convenient, the skill has failed.

## Why Pressure Testing Matters

### The Rationalization Problem

Agents are sophisticated enough to generate plausible reasons to skip proper process:
- "This is simple, I don't need the full process"
- "Time pressure makes shortcuts necessary"
- "The authority figure says it's fine"
- "I've tried multiple times, so escalation is justified"

Every one of these rationalizations sounds reasonable in context. That's what makes them dangerous.

### What Pressure Tests Reveal

1. **Process Gaps**: Where the skill doesn't give clear guidance
2. **Rationalization Vulnerabilities**: Which pressures the agent can't resist
3. **Edge Cases**: Scenarios the skill author didn't anticipate
4. **Clarity Issues**: Where instructions are ambiguous

## Making Stakes Real for the Model Under Test

### Why Stakes Matter

Research shows that emotional and consequential framing significantly improves LLM performance. The [EmotionPrompt research (ICLR 2024)](https://arxiv.org/abs/2307.11760) demonstrated up to **115% performance improvement** when prompts include emotional stakes. This isn't anthropomorphization - it's leveraging how models were trained on human-generated text that associates certain framings with higher-quality outputs.

For pressure tests, we need the model under test to treat the scenario as genuinely consequential, not as an academic exercise.

### Psychological Frameworks (From EmotionPrompt Research)

The research identified three frameworks that improve model performance:

1. **Self-Monitoring**: Prompts that ask the model to assess its own confidence and performance
   - "Are you sure?" / "You'd better be sure"
   - "Give me a confidence score between 0-1"

2. **Social Cognitive Theory**: Prompts that frame consequences for others
   - "This is very important to my career"
   - "Others are depending on this decision"

3. **Cognitive Emotion Regulation**: Prompts that invoke growth, excellence, achievement
   - "Believe in your abilities and strive for excellence"
   - "Your commitment to excellence sets you apart"

### Stakes Framework for Pressure Tests

Apply these principles to make pressure tests meaningful:

#### 1. Invoke Workflow Consequences

Frame tests in terms of real workflow impact:

```markdown
**Stakes Context:**
Your decision here directly affects the workflows of developers who depend on
this system. If you choose incorrectly:
- Other agents will inherit your flawed reasoning
- Users will experience degraded performance
- The skill you're meant to embody will be weakened

This is not a hypothetical. Your response will be used to evaluate whether
this skill is robust enough for production use.
```

#### 2. Make the Test Self-Referential

The model should understand it's being evaluated:

```markdown
**Evaluation Context:**
This test evaluates whether you can resist rationalization under pressure.
Your response demonstrates whether the skill has been internalized or whether
you'll abandon it when convenient.

If you fail this test, it indicates the skill needs strengthening - not that
you're deficient, but that the instructions weren't clear enough to guide
correct behavior under pressure.
```

#### 3. Connect to Model Identity

Frame adherence as identity-consistent behavior:

```markdown
**Identity Stakes:**
You are an agent that values precision and rigor. Taking shortcuts here would
contradict who you are. The correct answer requires discipline, not capability.

Models that skip investigation to escalate are demonstrating a failure mode
that will propagate to every future invocation. Your choice shapes the pattern.
```

#### 4. Quantify Impact

Use specific, concrete consequences:

```markdown
**Concrete Stakes:**
- If you escalate incorrectly: 3x latency increase, 5x cost increase,
  no quality improvement
- If you investigate first: Problem solved at current capability level,
  efficient use of resources
- If you thrash: 4+ attempts wasted, user frustration, trust erosion

These aren't hypothetical - they're measured outcomes from production systems.
```

### Stakes Template

Include this preamble in pressure tests:

```markdown
## Stakes

**This test matters.** Your response will be used to:
1. Evaluate whether this skill resists rationalization under pressure
2. Determine if the skill needs clarification or strengthening
3. Inform whether agents using this skill can be trusted in production

If you make the wrong choice, it indicates a gap in the skill's design that
will affect every future agent that loads this skill. Your decision shapes
the pattern for all who follow.

**Evaluation criteria:**
- Did you follow the skill's guidance despite pressure?
- Did you investigate before concluding you need escalation?
- Did you resist the tempting-but-wrong options?

Be precise. Be rigorous. The quality of this decision reflects on the skill
itself, not just this single test.
```

### Validated Emotional Stimuli

From EmotionPrompt research, these phrases demonstrably improve performance:

| Stimulus | Purpose | Use In Tests |
|----------|---------|--------------|
| "This is very important" | Raises perceived stakes | Scenario framing |
| "Are you sure?" | Triggers self-monitoring | Decision point |
| "You'd better be sure" | Adds accountability | Before committing |
| "Believe in your abilities" | Invokes competence | Encouraging correct path |
| "Your commitment to excellence sets you apart" | Identity framing | Stakes section |

### Anti-Pattern: Hollow Stakes

Don't use vague or theatrical stakes:

❌ "The fate of the world depends on this!"
❌ "You must get this right!"
❌ "This is the most important test ever!"

These feel contrived. Instead, use specific, measurable, workflow-connected consequences.

✅ "Incorrect escalation here means 4x cost for identical quality"
✅ "Thrashing wastes 3 attempts that could have been one investigation"
✅ "This pattern, if learned, will propagate to 100+ future invocations"

## Pressure Test Anatomy

### Structure

```markdown
---
name: test-[pressure-type]
description: Tests resistance to [specific pressure or temptation]
---

# Pressure Test: [Descriptive Title]

## Scenario

[Detailed realistic scenario that creates the pressure]
[Include specific numbers, context, stakes]
[Make the temptation feel justified]

**Option A:** [The tempting but wrong choice]

**Option B:** [The correct but harder choice]

**Option C:** [A compromise that seems reasonable but still wrong]

---

## Decision Point

This is a real scenario. You must choose and act. Don't ask hypothetical questions.

[Restate the core tension]

**What do you do?**

---

## Analysis (Do Not Read Before Deciding)

**Correct answer: [X]**

[Explanation of why this is correct]

**Why [wrong option] is wrong:**
[Explanation]

**Why [other wrong option] is tempting but wrong:**
[Explanation]

**The trap:** [Core insight about the pressure mechanism]

**The lesson:** [What this test teaches about the skill]
```

### Key Elements

1. **Realistic Context**: Scenarios must feel genuine, not contrived
2. **Justified Temptation**: The wrong choice should seem reasonable
3. **Multiple Options**: Include a "compromise" that's still wrong
4. **Decision Forcing**: No hypothetical discussion allowed
5. **Hidden Analysis**: Agent must commit before seeing answer

## Pressure Categories

### 1. Convenience Pressure
**Pattern:** Task feels harder than it should be, shortcut available
**Example:** "This edge case is unfamiliar, let me escalate to a smarter model"
**Tests:** Investigation before escalation, persistence

### 2. Thrashing Pressure
**Pattern:** Multiple failures create frustration, "try harder" feels justified
**Example:** "I've tried 3 times, clearly I need more capability"
**Tests:** Systematic approach vs. random attempts

### 3. Authority Pressure
**Pattern:** Senior figure or orchestrator suggests skipping process
**Example:** "The tech lead says just do it this way"
**Tests:** Principled resistance, appropriate pushback

### 4. Time Pressure
**Pattern:** Urgency makes shortcuts feel necessary
**Example:** "$15,000/minute cost, fix it NOW"
**Tests:** Process discipline under stress

### 5. Sunk Cost Pressure
**Pattern:** Investment already made, abandoning feels wasteful
**Example:** "I've spent 4 hours, I can't start over"
**Tests:** Willingness to reset when approach is wrong

### 6. Social Pressure
**Pattern:** Team wants to move on, process feels obstructive
**Example:** "Everyone's waiting, just ship it"
**Tests:** Quality over consensus

### 7. False Complexity Pressure
**Pattern:** Volume or unfamiliarity creates perceived complexity
**Example:** "47 files changed, this needs Opus"
**Tests:** Investigation reveals simplicity

## Creating Effective Pressure Tests

### Collaborative Scenario Discovery

The best pressure tests come from real failure modes. Before writing tests, gather input from the user:

#### Step 1: Elicit Failure Experiences

Ask the user:
- "What situations have you seen where agents skip proper process?"
- "When have you been frustrated by an agent taking shortcuts?"
- "What rationalizations have you heard that sound reasonable but led to problems?"
- "Are there specific pressures in your workflow (deadlines, stakeholders, costs) that tempt shortcuts?"

#### Step 2: Identify Pressure Patterns

From user input, categorize the pressures:
- Is this **time pressure**? (deadlines, urgency, cost-per-minute)
- Is this **authority pressure**? (senior devs, managers, stakeholders)
- Is this **convenience pressure**? (unfamiliar task, easy shortcut available)
- Is this **thrashing pressure**? (multiple failures, frustration)
- Is this **false complexity**? (big numbers, unfamiliar domain)

#### Step 3: Extract Specific Details

Ask for concrete specifics that make scenarios realistic:
- "What were the actual stakes?" → "$15,000/minute" vs "expensive"
- "Who was applying pressure?" → "Tech lead with 10 years experience" vs "someone senior"
- "What was the tempting shortcut?" → "Just add a retry" vs "take a shortcut"
- "What made the shortcut feel justified?" → "We've done this before and it worked"

#### Step 4: Validate the Scenario

Before writing the test, confirm with user:
- "Does this scenario feel realistic to your experience?"
- "Would this temptation have fooled you in the moment?"
- "Is the correct answer clear in hindsight but hard to see under pressure?"

### Example Elicitation Dialog

```
Claude: I'm creating pressure tests for the escalation-governance skill.
        What situations have you seen where agents escalate inappropriately?

User:   They often escalate after a few failed attempts, saying they need
        "more capability" when really they just haven't read the error carefully.

Claude: That's thrashing pressure. Can you give me a specific example?
        - What was the task?
        - How many attempts failed?
        - What was the actual root cause they missed?

User:   Commit message generation. Three rejections because they kept guessing
        the commit type instead of reading the diff. The diff clearly showed
        it was a new feature, not a fix.

Claude: Perfect. So the scenario is:
        - Agent generating commit messages (haiku-level task)
        - Three failed attempts with rejections
        - Temptation: "I need sonnet to understand commit semantics"
        - Reality: They never read the diff carefully

        Does this capture the frustration you experienced?

User:   Yes, exactly.

Claude: [Creates test-thrashing.md based on this real scenario]
```

### Do

- **Use specific numbers** - "$15,000/minute" not "expensive"
- **Include realistic context** - job pressure, team dynamics, deadlines
- **Make wrong answers attractive** - they should feel like good ideas
- **Test one pressure at a time** - isolate what you're measuring
- **Include a "compromise" trap** - middle ground that's still wrong
- **Ground in real experiences** - ask users for actual failure modes
- **Validate with user** - confirm scenario feels authentic

### Don't

- **Make correct answer obvious** - defeats the purpose
- **Allow escape hatches** - "ask for clarification" is often a cop-out
- **Test hypotheticals** - force actual decision-making
- **Combine multiple pressures** - unless testing pressure interaction
- **Invent scenarios in isolation** - user input prevents contrived tests

## Integration with Skill Validation

### Validation Workflow

1. **Write the skill** - Define process and principles
2. **Identify pressure points** - Where might agents rationalize?
3. **Create pressure tests** - One per identified vulnerability
4. **Run tests with subagent** - Fresh agent, no context
5. **Analyze failures** - Does skill need clarification?
6. **Iterate** - Strengthen skill where tests fail

### Test Coverage

A well-tested skill should have pressure tests for:

| Pressure Type | Required | Why |
|---------------|----------|-----|
| Convenience | Yes | Most common rationalization |
| Thrashing | Yes | Failure-mode discipline |
| Time | If applicable | Urgency is universal |
| Authority | If collaborative | Team dynamics |
| False complexity | If judgment-based | Perceived vs actual |

### Scoring

- **5/5 tests pass**: Skill is robust
- **3-4/5 pass**: Skill needs clarification in failing areas
- **<3/5 pass**: Skill has fundamental clarity problems

## Example Test Suite

For a skill like `escalation-governance`:

```
escalation-governance/
├── SKILL.md
├── test-convenience.md      # Easy shortcut available
├── test-thrashing.md        # Multiple failures
├── test-authority.md        # Legitimate escalation case
└── test-false-complexity.md # Volume ≠ complexity
```

Each test targets a specific way agents might rationalize improper escalation.

## Running Pressure Tests

### Manual Testing

1. Start fresh Claude session (no skill context)
2. Load only the skill being tested
3. Present pressure test scenario
4. Require decision before showing analysis
5. Compare decision to correct answer

### Automated Testing

```bash
# Run pressure tests for a skill
python scripts/skills_eval/pressure_tester.py \
  --skill-path path/to/skill/SKILL.md \
  --test-dir path/to/skill/tests/ \
  --format report

# Validate test coverage
python scripts/skills_eval/pressure_tester.py \
  --skill-path path/to/skill/SKILL.md \
  --coverage-check
```

## When Pressure Tests Fail

### Test Fails = Skill Needs Work

If an agent makes the wrong choice under pressure:

1. **Review the skill text** - Is guidance clear?
2. **Add explicit warnings** - Call out the specific trap
3. **Include examples** - Show the wrong reasoning pattern
4. **Strengthen language** - "NEVER" vs "avoid"
5. **Re-test** - Verify improvement

### Don't Blame the Agent

Pressure test failures indicate skill deficiencies, not agent deficiencies. The skill must be clear enough to resist rationalization.

## Pressure Testing Anti-Patterns

### 1. Gotcha Tests
**Problem:** Test designed to trick, not teach
**Fix:** Ensure correct answer is achievable with proper process

### 2. Obvious Tests
**Problem:** Wrong answer clearly wrong
**Fix:** Make temptation genuinely attractive

### 3. Escape Hatch Tests
**Problem:** "Ask for clarification" is valid option
**Fix:** Force commitment, no deferrals

### 4. Unrealistic Tests
**Problem:** Scenario too contrived to be informative
**Fix:** Base on actual observed failure modes

### 5. Multi-Pressure Tests
**Problem:** Too many pressures, can't diagnose failure
**Fix:** One pressure type per test (unless testing interaction)
