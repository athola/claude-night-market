#!/usr/bin/env python3
"""Post skill learnings to GitHub Discussions for collective intelligence.

Reads LEARNINGS.md (generated by aggregate_skill_logs.py) and posts a summary
to athola/claude-night-market GitHub Discussions in the "Learnings" category.

Part of Issue #69 Phase 6a: Collective Intelligence Loop

Features:
- Deduplication by date-based title pattern [Learning] YYYY-MM-DD
- Opt-out via ~/.claude/skills/discussions/config.json
- Graceful failure (warns to stderr, exits 0)
- Caches repo node ID to avoid repeated lookups
"""

from __future__ import annotations

import json
import re
import subprocess  # nosec B404
import sys
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

# Hardcoded target repository
TARGET_REPO = "athola/claude-night-market"
TARGET_OWNER = "athola"
TARGET_NAME = "claude-night-market"

# Known Discussion category ID for "Learnings"
LEARNINGS_CATEGORY_ID = "DIC_kwDOQbN88M4C2zJo"


def get_config_dir() -> Path:
    """Get the discussions config directory."""
    return Path.home() / ".claude" / "skills" / "discussions"


def get_learnings_path() -> Path:
    """Get path to LEARNINGS.md file."""
    return Path.home() / ".claude" / "skills" / "LEARNINGS.md"


@dataclass
class DiscussionConfig:
    """Configuration for collective intelligence posting."""

    auto_post_learnings: bool = True
    target_repo: str = TARGET_REPO
    promotion_threshold: int = 3
    promotion_emoji: str = "\U0001f525"  # fire emoji

    @classmethod
    def load(cls) -> DiscussionConfig:
        """Load config from disk, falling back to defaults."""
        config_path = get_config_dir() / "config.json"
        if config_path.exists():
            try:
                data = json.loads(config_path.read_text())
                return cls(
                    auto_post_learnings=data.get("auto_post_learnings", True),
                    target_repo=data.get("target_repo", TARGET_REPO),
                    promotion_threshold=data.get("promotion_threshold", 3),
                    promotion_emoji=data.get("promotion_emoji", "\U0001f525"),
                )
            except (json.JSONDecodeError, OSError) as e:
                print(
                    f"Warning: Could not load config: {e}",
                    file=sys.stderr,
                )
        return cls()


@dataclass
class PostedRecord:
    """Tracks posted discussions to avoid duplicates."""

    posted: dict[str, str] = field(default_factory=dict)  # title -> discussion_url
    repo_node_id: str = ""

    @classmethod
    def load(cls) -> PostedRecord:
        """Load posted record from disk."""
        record_path = get_config_dir() / "posted.json"
        if record_path.exists():
            try:
                data = json.loads(record_path.read_text())
                return cls(
                    posted=data.get("posted", {}),
                    repo_node_id=data.get("repo_node_id", ""),
                )
            except (json.JSONDecodeError, OSError):
                pass
        return cls()

    def save(self) -> None:
        """Save posted record to disk."""
        record_path = get_config_dir() / "posted.json"
        record_path.parent.mkdir(parents=True, exist_ok=True)
        record_path.write_text(
            json.dumps(
                {"posted": self.posted, "repo_node_id": self.repo_node_id},
                indent=2,
            )
        )

    def is_posted(self, title: str) -> bool:
        """Check if a discussion with this title was already posted."""
        return title in self.posted


@dataclass
class LearningSummary:
    """Parsed summary from LEARNINGS.md."""

    last_updated: str = ""
    analysis_period: str = ""
    skills_analyzed: int = 0
    total_executions: int = 0
    high_impact_issues: list[dict[str, Any]] = field(default_factory=list)
    slow_skills: list[dict[str, Any]] = field(default_factory=list)
    low_rated_skills: list[dict[str, Any]] = field(default_factory=list)


def _extract_metadata_field(content: str, field: str) -> str:
    """Extract a bold metadata field value from content."""
    match = re.search(rf"\*\*{re.escape(field)}\*\*:\s*(.+)", content)
    return match.group(1).strip() if match else ""


def _extract_bold_field(text: str, field: str) -> str:
    """Extract a bold field value from a subsection body."""
    match = re.search(rf"\*\*{re.escape(field)}\*\*:\s*(.+)", text)
    return match.group(1).strip() if match else ""


def parse_learnings_md(content: str) -> LearningSummary:
    """Parse LEARNINGS.md into structured summary.

    Args:
        content: Raw markdown content of LEARNINGS.md

    Returns:
        LearningSummary with extracted data

    """
    summary = LearningSummary()

    # Extract metadata from header
    summary.last_updated = _extract_metadata_field(content, "Last Updated")
    summary.analysis_period = _extract_metadata_field(content, "Analysis Period")

    skills_str = _extract_metadata_field(content, "Skills Analyzed")
    summary.skills_analyzed = int(skills_str) if skills_str.isdigit() else 0

    exec_str = _extract_metadata_field(content, "Total Executions")
    summary.total_executions = int(exec_str) if exec_str.isdigit() else 0

    # Extract High-Impact Issues section
    hi_section = _extract_section(content, "## High-Impact Issues")
    if hi_section:
        summary.high_impact_issues = _parse_high_impact_issues(hi_section)

    # Extract Slow Execution section (table rows)
    slow_section = _extract_section(content, "## Slow Execution")
    if slow_section:
        for match in re.finditer(
            r"\|\s*`([^`]+)`\s*\|\s*([^\|]+)\s*\|\s*([^\|]+)\s*\|\s*(\d+)\s*\|",
            slow_section,
        ):
            summary.slow_skills.append(
                {
                    "skill": match.group(1).strip(),
                    "avg_duration": match.group(2).strip(),
                    "max_duration": match.group(3).strip(),
                    "executions": int(match.group(4).strip()),
                }
            )

    # Extract Low User Ratings section
    lr_section = _extract_section(content, "## Low User Ratings")
    if lr_section:
        for match in re.finditer(
            r"### (.+?)\s*-\s*([\d.]+)/5\.0",
            lr_section,
        ):
            summary.low_rated_skills.append(
                {
                    "skill": match.group(1).strip(),
                    "rating": float(match.group(2)),
                }
            )

    return summary


def _parse_high_impact_issues(section: str) -> list[dict[str, Any]]:
    """Parse high-impact issues from a section body."""
    issues: list[dict[str, Any]] = []
    for match in re.finditer(
        r"### (.+?)\n(.*?)(?=\n### |\n---|\n## |\Z)",
        section,
        re.DOTALL,
    ):
        skill_name = match.group(1).strip()
        body = match.group(2).strip()
        issue: dict[str, Any] = {"skill": skill_name}

        for field_name in ("Type", "Severity", "Metric"):
            value = _extract_bold_field(body, field_name)
            if value:
                issue[field_name.lower()] = value

        issues.append(issue)
    return issues


def _extract_section(content: str, heading: str) -> str | None:
    """Extract content between a heading and the next same-level heading or ---."""
    pattern = re.escape(heading) + r"\n(.*?)(?=\n## |\n---|\Z)"
    match = re.search(pattern, content, re.DOTALL)
    if match:
        return match.group(1).strip()
    return None


def format_discussion_body(summary: LearningSummary) -> str:
    """Format a Discussion post body from parsed learnings.

    Args:
        summary: Parsed learning summary

    Returns:
        Markdown-formatted discussion body

    """
    lines: list[str] = []

    lines.append("## Summary Stats")
    lines.append("")
    lines.append(f"- **Analysis Period**: {summary.analysis_period}")
    lines.append(f"- **Skills Analyzed**: {summary.skills_analyzed}")
    lines.append(f"- **Total Executions**: {summary.total_executions}")
    lines.append(f"- **Last Updated**: {summary.last_updated}")
    lines.append("")

    if summary.high_impact_issues:
        lines.append("## Top Issues")
        lines.append("")
        for issue in summary.high_impact_issues[:5]:
            severity = issue.get("severity", "unknown")
            metric = issue.get("metric", "")
            issue_type = issue.get("type", "")
            lines.append(
                f"- **{issue['skill']}** [{severity}]: {metric} ({issue_type})"
            )
        lines.append("")

    if summary.slow_skills:
        lines.append("## Slow Execution Patterns")
        lines.append("")
        for slow in summary.slow_skills[:5]:
            lines.append(
                f"- `{slow['skill']}`: avg {slow['avg_duration']}, "
                f"max {slow['max_duration']} ({slow['executions']} runs)"
            )
        lines.append("")

    if summary.low_rated_skills:
        lines.append("## Low-Rated Skills")
        lines.append("")
        for low in summary.low_rated_skills[:5]:
            lines.append(f"- `{low['skill']}`: {low['rating']}/5.0")
        lines.append("")

    lines.append("---")
    lines.append("*Auto-posted by Phase 6a Collective Intelligence Loop (Issue #69)*")
    lines.append("")
    lines.append(
        "React with \U0001f525 if you've experienced similar issues. "
        "Items with 3+ reactions will be promoted to Issues."
    )

    return "\n".join(lines)


def run_gh_graphql(query: str, variables: dict[str, Any] | None = None) -> Any:
    """Run a GraphQL query via gh api.

    Args:
        query: GraphQL query string
        variables: Optional query variables

    Returns:
        Parsed JSON response

    Raises:
        RuntimeError: If gh command fails

    """
    cmd = ["gh", "api", "graphql"]

    # Build the request body
    body: dict[str, Any] = {"query": query}
    if variables:
        body["variables"] = variables

    cmd.extend(["-f", f"query={query}"])
    if variables:
        for key, value in variables.items():
            cmd.extend(["-f", f"{key}={value}"])

    result = subprocess.run(  # noqa: S603, S607  # nosec B603
        cmd,
        capture_output=True,
        text=True,
        timeout=30,
        check=False,
    )

    if result.returncode != 0:
        raise RuntimeError(
            f"gh api graphql failed (exit {result.returncode}): {result.stderr}"
        )

    return json.loads(result.stdout)


def get_repo_node_id(record: PostedRecord) -> str:
    """Get the repository node ID, using cache if available.

    Args:
        record: PostedRecord to cache the ID in

    Returns:
        Repository node ID string

    """
    if record.repo_node_id:
        return record.repo_node_id

    query = (
        f'query {{ repository(owner: "{TARGET_OWNER}",'
        f' name: "{TARGET_NAME}") {{ id }} }}'
    )

    response = run_gh_graphql(query)
    node_id = response["data"]["repository"]["id"]
    record.repo_node_id = node_id
    record.save()
    return str(node_id)


def check_existing_discussion(title: str) -> str | None:
    """Check if a discussion with this title already exists.

    Args:
        title: Discussion title to search for

    Returns:
        Discussion URL if found, None otherwise

    """
    escaped_title = title.replace('"', '\\"')
    query = (
        f'query {{ search(query: "repo:{TARGET_REPO} in:title \\"{escaped_title}\\"", '
        "type: DISCUSSION, first: 5) { nodes { ... on Discussion "
        "{ title url } } } }"  # noqa: UP031
    )

    try:
        response = run_gh_graphql(query)
        nodes = response.get("data", {}).get("search", {}).get("nodes", [])
        for node in nodes:
            if node.get("title") == title:
                return str(node.get("url", ""))
    except (RuntimeError, KeyError, json.JSONDecodeError):
        pass

    return None


def create_discussion(
    repo_id: str,
    category_id: str,
    title: str,
    body: str,
) -> str:
    """Create a GitHub Discussion.

    Args:
        repo_id: Repository node ID
        category_id: Discussion category node ID
        title: Discussion title
        body: Discussion body (markdown)

    Returns:
        URL of the created discussion

    """
    query = """
    mutation($repoId: ID!, $categoryId: ID!, $title: String!, $body: String!) {
      createDiscussion(input: {
        repositoryId: $repoId,
        categoryId: $categoryId,
        title: $title,
        body: $body
      }) {
        discussion {
          url
        }
      }
    }
    """

    response = run_gh_graphql(
        query,
        variables={
            "repoId": repo_id,
            "categoryId": category_id,
            "title": title,
            "body": body,
        },
    )

    return str(response["data"]["createDiscussion"]["discussion"]["url"])


def _load_and_validate_learnings(path: Path) -> LearningSummary | None:
    """Load LEARNINGS.md, validate it has content worth posting."""
    if not path.exists():
        print(
            f"Warning: {path} not found. Run aggregate-logs first.",
            file=sys.stderr,
        )
        return None

    content = path.read_text()
    if not content.strip():
        print("Warning: LEARNINGS.md is empty.", file=sys.stderr)
        return None

    summary = parse_learnings_md(content)
    if summary.skills_analyzed == 0 and summary.total_executions == 0:
        print("No learnings to post (empty analysis).", file=sys.stderr)
        return None

    return summary


def post_learnings(learnings_path: Path | None = None) -> str | None:
    """Main entry point: parse LEARNINGS.md and post to Discussions.

    Args:
        learnings_path: Override path to LEARNINGS.md (for testing)

    Returns:
        Discussion URL if posted, None if skipped

    """
    # Load config
    config = DiscussionConfig.load()
    if not config.auto_post_learnings:
        print("Learnings posting disabled via config.", file=sys.stderr)
        return None

    # Read and validate LEARNINGS.md
    path = learnings_path or get_learnings_path()
    summary = _load_and_validate_learnings(path)
    if summary is None:
        return None

    # Generate title with date
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")  # noqa: UP017
    title = f"[Learning] {today}"

    # Check deduplication
    record = PostedRecord.load()
    if record.is_posted(title):
        print(
            f"Already posted for today ({title}). Skipping.",
            file=sys.stderr,
        )
        return record.posted[title]

    # Also check GitHub in case posted.json was lost
    existing_url = check_existing_discussion(title)
    if existing_url:
        print(
            f"Discussion already exists: {existing_url}. Skipping.",
            file=sys.stderr,
        )
        record.posted[title] = existing_url
        record.save()
        return existing_url

    # Get repo node ID (cached)
    repo_id = get_repo_node_id(record)

    # Format and post
    body = format_discussion_body(summary)
    url = create_discussion(repo_id, LEARNINGS_CATEGORY_ID, title, body)

    # Track posted discussion
    record.posted[title] = url
    record.save()

    print(f"Posted learning summary: {url}")
    return url


def main() -> None:
    """CLI entry point."""
    try:
        url = post_learnings()
        if url:
            print(f"\nDiscussion URL: {url}")
    except RuntimeError as e:
        print(f"Warning: Could not post learnings: {e}", file=sys.stderr)
        sys.exit(0)  # Graceful failure
    except FileNotFoundError:
        print(
            "Warning: gh CLI not found. Install GitHub CLI to enable "
            "collective intelligence features.",
            file=sys.stderr,
        )
        sys.exit(0)


if __name__ == "__main__":
    main()
