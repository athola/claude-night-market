#!/usr/bin/env python3
"""
Token Usage Estimator for Skills

A uv-compatible command-line tool for analyzing and estimating token usage
in skill files and their dependencies.

Usage:
    uv run token-estimator --file SKILL.md
    uv run token-estimator --directory skills/
"""

import os
import sys
import argparse
import re
from pathlib import Path
from typing import Dict, List, Tuple

# Rough token estimation constants
CHARS_PER_TOKEN = 4  # Average for English text
YAML_FRONTMASSIVE_TOKEN_MULTIPLIER = 1.2  # YAML is more token-dense
CODE_TOKEN_MULTIPLIER = 1.5  # Code blocks are more token-dense

def count_tokens(text: str) -> int:
    """Estimate token count from text using character-based heuristic"""
    return len(text) // CHARS_PER_TOKEN

def extract_frontmatter(content: str) -> Tuple[str, str]:
    """Extract YAML frontmatter from markdown content"""
    if content.startswith('---\n'):
        try:
            end_idx = content.index('\n---\n', 4)
            return content[:end_idx + 5], content[end_idx + 5:]
        except ValueError:
            return "", content
    return "", content

def extract_code_blocks(content: str) -> List[str]:
    """Extract code blocks from markdown content"""
    pattern = r'```[^`\n]*\n(.*?)\n```'
    return re.findall(pattern, content, re.DOTALL)

def analyze_file(file_path: Path, include_dependencies: bool = False) -> Dict:
    """Analyze a single skill file for token usage"""
    if not file_path.exists():
        raise FileNotFoundError(f"File not found: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Extract components
    frontmatter, body_content = extract_frontmatter(content)
    code_blocks = extract_code_blocks(body_content)

    # Calculate tokens for different components
    frontmatter_tokens = int(count_tokens(frontmatter) * YAML_FRONTMASSIVE_TOKEN_MULTIPLIER)
    body_tokens = count_tokens(body_content)
    code_tokens = int(sum(count_tokens(block) for block in code_blocks) * CODE_TOKEN_MULTIPLIER)

    # Extract dependencies from frontmatter
    dependencies = []
    if frontmatter:
        dep_match = re.search(r'dependencies:\s*\[(.*?)\]', frontmatter, re.DOTALL)
        if dep_match:
            deps_text = dep_match.group(1)
            dependencies = [dep.strip().strip('"\'') for dep in deps_text.split(',') if dep.strip()]

    total_tokens = frontmatter_tokens + body_tokens + code_tokens

    result = {
        'file': str(file_path),
        'total_tokens': total_tokens,
        'frontmatter_tokens': frontmatter_tokens,
        'body_tokens': body_tokens,
        'code_tokens': code_tokens,
        'dependencies': dependencies,
        'character_count': len(content),
        'line_count': len(content.splitlines())
    }

    # Add dependency analysis if requested
    if include_dependencies and dependencies:
        dependency_tokens = 0
        missing_deps = []

        for dep in dependencies:
            # Try to find dependency file
            dep_paths = [
                file_path.parent / f"{dep}.md",
                file_path.parent / dep / "SKILL.md",
                file_path.parent.parent / "skills" / dep / "SKILL.md"
            ]

            dep_file = None
            for path in dep_paths:
                if path.exists():
                    dep_file = path
                    break

            if dep_file:
                try:
                    dep_analysis = analyze_file(dep_file, include_dependencies=False)
                    dependency_tokens += dep_analysis['total_tokens']
                except Exception:
                    missing_deps.append(dep)
            else:
                missing_deps.append(dep)

        result['dependency_tokens'] = dependency_tokens
        result['missing_dependencies'] = missing_deps
        result['total_with_dependencies'] = total_tokens + dependency_tokens

    return result

def format_analysis(analysis: Dict, include_dependencies: bool = False) -> str:
    """Format analysis results for display"""
    lines = [
        f"=== {analysis['file']} ===",
        f"Total tokens: {analysis['total_tokens']:,}",
        f"Character count: {analysis['character_count']:,}",
        f"Line count: {analysis['line_count']:,}",
        "",
        "Component breakdown:",
        f"  Frontmatter: {analysis['frontmatter_tokens']:,} tokens",
        f"  Body content: {analysis['body_tokens']:,} tokens",
        f"  Code blocks: {analysis['code_tokens']:,} tokens",
    ]

    if analysis['dependencies']:
        lines.extend([
            "",
            f"Dependencies: {len(analysis['dependencies'])}",
            f"  {', '.join(analysis['dependencies'])}"
        ])

    if include_dependencies:
        if 'dependency_tokens' in analysis:
            lines.extend([
                "",
                f"Dependency tokens: {analysis['dependency_tokens']:,}",
                f"Total with dependencies: {analysis['total_with_dependencies']:,}"
            ])

        if analysis.get('missing_dependencies'):
            lines.extend([
                "",
                f"Missing dependencies: {len(analysis['missing_dependencies'])}",
                f"  {', '.join(analysis['missing_dependencies'])}"
            ])

    # Recommendations
    lines.extend(["", "=== Recommendations ==="])

    total = analysis.get('total_with_dependencies', analysis['total_tokens'])

    if total > 4096:
        lines.append("HIGH TOKEN USAGE: Consider modularization")
    elif total > 2048:
        lines.append("MODERATE TOKEN USAGE: Monitor for growth")
    else:
        lines.append("ACCEPTABLE TOKEN USAGE")

    if analysis['code_tokens'] > analysis['body_tokens']:
        lines.append("CONSIDER: Heavy code content - may benefit from tool extraction")

    return "\n".join(lines)

def main():
    parser = argparse.ArgumentParser(
        description="Estimate token usage for skill files",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  token-estimator SKILL.md
  token-estimator --file my-skill/SKILL.md --include-dependencies
  token-estimator --directory ~/.claude/skills/ --threshold 2048
        """
    )

    parser.add_argument('-f', '--file', type=Path, help='Skill file to analyze')
    parser.add_argument('-d', '--directory', type=Path, help='Directory containing skills')
    parser.add_argument('-i', '--include-dependencies', action='store_true',
                       help='Include dependency token analysis')
    parser.add_argument('-t', '--threshold', type=int, default=2048,
                       help='Token threshold for warnings (default: 2048)')
    parser.add_argument('-v', '--verbose', action='store_true',
                       help='Verbose output')

    args = parser.parse_args()

    if not args.file and not args.directory:
        parser.error("Either --file or --directory must be specified")

    try:
        if args.file:
            analysis = analyze_file(args.file, args.include_dependencies)
            print(format_analysis(analysis, args.include_dependencies))

        elif args.directory:
            if not args.directory.is_dir():
                print(f"Error: {args.directory} is not a directory", file=sys.stderr)
                sys.exit(1)

            # Find all skill files
            skill_files = list(args.directory.rglob("SKILL.md"))

            if not skill_files:
                print(f"No SKILL.md files found in {args.directory}")
                sys.exit(1)

            total_tokens = 0
            high_usage_files = []

            for skill_file in sorted(skill_files):
                try:
                    analysis = analyze_file(skill_file, args.include_dependencies)
                    print(format_analysis(analysis, args.include_dependencies))
                    print()

                    file_total = analysis.get('total_with_dependencies', analysis['total_tokens'])
                    total_tokens += file_total

                    if file_total > args.threshold:
                        high_usage_files.append((skill_file, file_total))

                except Exception as e:
                    print(f"Error analyzing {skill_file}: {e}", file=sys.stderr)
                    print()

            # Summary
            print("=== SUMMARY ===")
            print(f"Total skills analyzed: {len(skill_files)}")
            print(f"Combined tokens: {total_tokens:,}")

            if high_usage_files:
                print(f"\nHigh usage files (> {args.threshold:,} tokens):")
                for file_path, tokens in high_usage_files:
                    print(f"  {file_path}: {tokens:,} tokens")

    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
